---
title: "p8105_hw5_nk2814"
author: "Naama Kipperman"
date: "11/5/2019"
output: github_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(rvest)
library(readr)
library(patchwork)
library(ggplot2)

knitr::opts_chunk$set(echo = TRUE)

```

#### Problem 1 

Write function that replaces missing values with mean if numeric variable, and replaces missing values with "virginica" if character variable (Species).

```{r}

set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species)) 




replace_missing_vals = function(x){
  if (is.numeric(x))
  {x = replace_na(x, mean(x, na.rm=TRUE))}
  else if (is.character(x))
  {x = replace_na(x, "virginica")}
}


fixed_iris = map(iris_with_missing, replace_missing_vals) %>% 
  as_tibble()

```


#### Problem 2 


```{r}
p2_files = list.files(path="./hw5_data/data", full.names=TRUE) 
  
trial_data =
  p2_files %>% 
map_df(read.csv) %>%
  mutate(
    subj_id = c(1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10),
    study_arm = c("control", "control", "control", "control", "control", "control", "control", "control", "control", "control", "experimental", "experimental", "experimental", "experimental", "experimental", "experimental", "experimental", "experimental", "experimental", "experimental")
  ) %>% 
  select(subj_id, study_arm, everything()) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "values"
  ) 


trial_data %>% 
  group_by(study_arm) %>% 
  ggplot(aes(x=week, y=values, color=study_arm, group=subj_id)) + geom_point() + geom_path()


```

The experimental subjects in this trial appear to have higher values, on average, during the course of the study. 


#### Problem 3

**Part 1**: 

Fix following parameters:

* n=30

* xi1 as draws from a standard Normal distribution 

* β0=2 

* σ2=50

* β1=0

Generate 10,000 datasets from the model yi=β0+β1xi1+ϵi with ϵi∼N[0,σ2].
For each dataset, save β^1 and the p-value arising from a test of H:β1=0 using α=0.05.

```{r}
set.seed(10)

# write function that runs linear regression model and outputs B1 estimates and associated p-values

sim_regression = function(beta1, n=30, beta0 = 2) {
  
  sim_data = tibble(
    x = rnorm(30, mean = 0, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, mean=0, sd= 50^0.50)
  )
  
  ls_fit = lm(y ~ x, data = sim_data) %>% broom::tidy()
  
  tibble(
    b1_estimate = ls_fit[[2,2]],
    p_value = ls_fit[[2,5]]
  )
  
}

# generate 10,000 datasets 

sim_results1 = 
  rerun(10000, sim_regression(beta1=0)) %>% 
  bind_rows()


```



**Part 2**: Repeat the above for β1={1,2,3,4,5,6}

```{r}
sim_results2 = 
  tibble(b1_values = c(1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output_lists = map(.x = b1_values, ~rerun(10000, sim_regression(beta1 = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
```


**Part 3**: Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of β1 on the x axis. Describe the association between effect size and power.
```{r}

sim_results2 %>% 
  mutate(
    rejected = ifelse(p_value < 0.05, "rejected", "not rejected")
  ) %>%
  group_by(b1_values, rejected) %>% 
  summarize(n=n()) %>% 
  mutate(
    proportion = (n/sum(n))
  ) %>% 
  filter(rejected=="rejected") %>% 
  ggplot(aes(x=b1_values, y=proportion, fill=rejected)) + geom_col() + labs(x="Effect Size (Beta1)", y="Power")
  

```

As effect size increases, power increases - that is, the greater the effect size, the greater power we have to detect a difference. 


**Part 4**

Make a plot showing the average estimate of β^1 on the y axis and the true value of β1 on the x axis.

Make a second plot (or overlay on the first) the average estimate of β^1 only in samples for which the null was rejected on the y axis and the true value of β1 on the x axis. 


```{r}


# data frame with all estimates of B1
df_1=
sim_results2 %>% 
  group_by(b1_values) %>% 
  summarize(
    avg_estimate=mean(b1_estimate)) 

# data frame with B1 estimates only when null was rejected

df_2=
sim_results2 %>% 
    mutate(
    rejected = ifelse(p_value < 0.05, "rejected", "not rejected")
  ) %>%
  filter(rejected=="rejected") %>% 
  group_by(b1_values) %>% 
  summarize(
  avg_estimate=mean(b1_estimate)) 

# make plot with both data

ggplot(data=df_1, aes(x=b1_values, y=avg_estimate, color="all estimates of B1")) + geom_point()+geom_point(data=df_2, aes(x=b1_values, y=avg_estimate, color="estimates when null was rejected")) + labs(x="True Value of B1", y="Average Estimate of B1")
  


```


**Is the sample average of β^1 across tests for which the null is rejected approximately equal to the true value of β1? Why or why not?**

In tests where the null is rejected, there tends to be a discrepancy between the true value of B1 and the average estimate of B1, especially for lower effect sizes (true value of B1=1,2,3,4). For larger effect sizes such as when B1=5,6 the average estimate of B1 tends to more closely parallel the true value of B1. This is because the null is rejected more frequently at larger effect sizes (larger values of B1). 

